{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# E2E Machine Learning Workflow on Azure ML using the Python SDK v2 pt.1\n",
        "\n",
        "##### Model training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Learning Objectives** - By the end of this tutorial, you should be able to use Azure Machine Learning (Azure ML) to productionise your ML project.\n",
        "\n",
        "This means you will be able to leverage the AzureML Python SDK to:\n",
        "\n",
        "- connect to your Azure ML workspace\n",
        "- create Azure ML data assets\n",
        "- create reusable Azure ML components\n",
        "- create, validate and run Azure ML pipelines\n",
        "- deploy the newly-trained model as an endpoint\n",
        "- call the Azure ML endpoint for inferencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Motivations** - This tutorial is intended to introduce Azure ML to data scientists who want to scale up or publish their ML projects. By completing a familiar end-to-end project, which starts by loading the data and ends by creating and calling an online inference endpoint, the user should become familiar with the core concepts of Azure ML and their most common usage. Each step of this tutorial can be modified or performed in other ways that might have security or scalability advantages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Requirements** - In order to benefit from this tutorial, you need to have:\n",
        "- basic understanding of Machine Learning projects workflow\n",
        "- an Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
        "- a working Azure ML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
        "- a Python environmnet\n",
        "- [installed Azure Machine Learning Python SDK v2](https://github.com/Azure/azureml-examples/blob/sdk-preview/sdk/setup.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, you'll create an Azure ML pipeline to train a model for credit default prediction. The pipeline handles the data cleaning, preparation, training and registering the trained model. You'll then run the pipeline, deploy the model and use it.\n",
        "\n",
        "### Set up the pipeline resources\n",
        "\n",
        "The Azure ML framework can be used from CLI, Python SDK, or studio interface. In this example, you'll use the AzureML Python SDK v2 to create a pipeline. \n",
        "\n",
        "Before creating the pipeline, you'll set up the resources the pipeline will use:\n",
        "\n",
        "* The dataset for training\n",
        "* The software environment to run the pipeline\n",
        "\n",
        "### Connect to the workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "We are using `DefaultAzureCredential` to get access to workspace. \n",
        "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
        "\n",
        "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 1: Set-up - client handle & data exploration\n",
        "This section sets up the MLClient object and tests the connection to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1713272941935
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "subscription_id = ''\n",
        "resource_group = ''\n",
        "workspace = ''\n",
        "\n",
        "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "> [!IMPORTANT]\n",
        "> Creating MLClient will not connect to the workspace. The client initialization is lazy, it will wait for the first time it needs to make a call (in the notebook below, that will happen during dataset registration).\n",
        "\n",
        "## Register data from a local file\n",
        "\n",
        "The data you use for training is usually in one of the locations below:\n",
        "\n",
        "* Local machine\n",
        "* Web\n",
        "* Big Data Storage services (for example, Azure Blob, Azure Data Lake Storage, SQL)\n",
        " \n",
        "Azure ML uses a [`Data`](https://docs.microsoft.com/azure/machine-learning/how-to-create-register-data-assets?tabs=Python-SDK) object to register a reusable definition of data, and consume data within a pipeline. In the section below, you'll consume some data from web url as one example. `Data` assets ets from other sources can be created as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1713273091178
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'credit_card_default_data', 'description': 'Dataset for credit card defaults', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/876b91eb-54d6-4433-af3b-5c9914d5ccea/resourceGroups/ej_vision_playground/providers/Microsoft.MachineLearningServices/workspaces/ej-workshop-workspace/data/credit_card_default_data/versions/3', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/jonesethan2/code/Users/jonesethan/AML_workshop/e2e_ml_workflow/credit_card_training_pipe', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f8b74be0e50>, 'serialize': <msrest.serialization.Serializer object at 0x7f8b60bed900>, 'version': '3', 'latest_version': None, 'path': 'azureml://subscriptions/876b91eb-54d6-4433-af3b-5c9914d5ccea/resourcegroups/ej_vision_playground/workspaces/ej-workshop-workspace/datastores/workspaceblobstore/paths/LocalUpload/b3ca843612e2ff4c8fab1474c9d0bb1a/credit_card_default_data_v1.csv', 'datastore': None})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "credit_data = Data(\n",
        "    name=\"credit_card_default_data\",\n",
        "    path=\"../data/credit_card_default_data_v1.csv\",\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"Dataset for credit card defaults\",\n",
        "    version=\"1\",\n",
        ")\n",
        "\n",
        "ml_client.data.create_or_update(credit_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1713273165060
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data asset URI: azureml://subscriptions/876b91eb-54d6-4433-af3b-5c9914d5ccea/resourcegroups/ej_vision_playground/workspaces/ej-workshop-workspace/datastores/workspaceblobstore/paths/LocalUpload/b3ca843612e2ff4c8fab1474c9d0bb1a/credit_card_default_data_v1.csv\n"
          ]
        }
      ],
      "source": [
        "credit_data = ml_client.data.get(name=\"credit_card_default_data\", version=\"1\")\n",
        "print(f\"Data asset URI: {credit_data.path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1713273202173
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_df = pd.read_csv(credit_data.path, header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1713273257585
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>...</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <th>default payment next month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2500.00000</td>\n",
              "      <td>2500.00000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.00000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.00000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2.500000e+03</td>\n",
              "      <td>2500.0000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "      <td>2500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>28750.50000</td>\n",
              "      <td>165392.00000</td>\n",
              "      <td>1.636800</td>\n",
              "      <td>1.929600</td>\n",
              "      <td>1.52880</td>\n",
              "      <td>35.807600</td>\n",
              "      <td>-0.057600</td>\n",
              "      <td>-0.15680</td>\n",
              "      <td>-0.188000</td>\n",
              "      <td>-0.240000</td>\n",
              "      <td>...</td>\n",
              "      <td>43291.162800</td>\n",
              "      <td>38652.274800</td>\n",
              "      <td>37710.555200</td>\n",
              "      <td>6447.848400</td>\n",
              "      <td>6.441450e+03</td>\n",
              "      <td>6233.4596</td>\n",
              "      <td>4774.937600</td>\n",
              "      <td>5024.656400</td>\n",
              "      <td>5487.661600</td>\n",
              "      <td>0.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>721.83216</td>\n",
              "      <td>127514.98094</td>\n",
              "      <td>0.481018</td>\n",
              "      <td>0.832419</td>\n",
              "      <td>0.52429</td>\n",
              "      <td>9.330799</td>\n",
              "      <td>1.127648</td>\n",
              "      <td>1.21425</td>\n",
              "      <td>1.201513</td>\n",
              "      <td>1.231506</td>\n",
              "      <td>...</td>\n",
              "      <td>64202.771958</td>\n",
              "      <td>59876.673254</td>\n",
              "      <td>59390.266423</td>\n",
              "      <td>27426.696364</td>\n",
              "      <td>3.778192e+04</td>\n",
              "      <td>25715.8828</td>\n",
              "      <td>18620.056749</td>\n",
              "      <td>18159.838678</td>\n",
              "      <td>19088.935962</td>\n",
              "      <td>0.412148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>27501.00000</td>\n",
              "      <td>10000.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.00000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-50616.000000</td>\n",
              "      <td>-53007.000000</td>\n",
              "      <td>-94625.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>28125.75000</td>\n",
              "      <td>50000.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1923.000000</td>\n",
              "      <td>1606.500000</td>\n",
              "      <td>1050.000000</td>\n",
              "      <td>969.250000</td>\n",
              "      <td>7.367500e+02</td>\n",
              "      <td>646.7500</td>\n",
              "      <td>286.500000</td>\n",
              "      <td>179.000000</td>\n",
              "      <td>239.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>28750.50000</td>\n",
              "      <td>140000.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>18474.500000</td>\n",
              "      <td>16520.500000</td>\n",
              "      <td>15830.000000</td>\n",
              "      <td>2100.000000</td>\n",
              "      <td>2.050000e+03</td>\n",
              "      <td>2000.0000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>29375.25000</td>\n",
              "      <td>230000.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>55975.750000</td>\n",
              "      <td>49844.500000</td>\n",
              "      <td>48498.250000</td>\n",
              "      <td>5010.250000</td>\n",
              "      <td>5.000000e+03</td>\n",
              "      <td>4704.5000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>4100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>30000.00000</td>\n",
              "      <td>780000.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>504474.000000</td>\n",
              "      <td>587067.000000</td>\n",
              "      <td>498316.000000</td>\n",
              "      <td>873552.000000</td>\n",
              "      <td>1.227082e+06</td>\n",
              "      <td>889043.0000</td>\n",
              "      <td>621000.000000</td>\n",
              "      <td>426529.000000</td>\n",
              "      <td>443001.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID     LIMIT_BAL          SEX    EDUCATION    MARRIAGE  \\\n",
              "count   2500.00000    2500.00000  2500.000000  2500.000000  2500.00000   \n",
              "mean   28750.50000  165392.00000     1.636800     1.929600     1.52880   \n",
              "std      721.83216  127514.98094     0.481018     0.832419     0.52429   \n",
              "min    27501.00000   10000.00000     1.000000     1.000000     0.00000   \n",
              "25%    28125.75000   50000.00000     1.000000     1.000000     1.00000   \n",
              "50%    28750.50000  140000.00000     2.000000     2.000000     2.00000   \n",
              "75%    29375.25000  230000.00000     2.000000     2.000000     2.00000   \n",
              "max    30000.00000  780000.00000     2.000000     6.000000     3.00000   \n",
              "\n",
              "               AGE        PAY_0       PAY_2        PAY_3        PAY_4  ...  \\\n",
              "count  2500.000000  2500.000000  2500.00000  2500.000000  2500.000000  ...   \n",
              "mean     35.807600    -0.057600    -0.15680    -0.188000    -0.240000  ...   \n",
              "std       9.330799     1.127648     1.21425     1.201513     1.231506  ...   \n",
              "min      21.000000    -2.000000    -2.00000    -2.000000    -2.000000  ...   \n",
              "25%      28.000000    -1.000000    -1.00000    -1.000000    -1.000000  ...   \n",
              "50%      34.000000     0.000000     0.00000     0.000000     0.000000  ...   \n",
              "75%      42.000000     0.000000     0.00000     0.000000     0.000000  ...   \n",
              "max      74.000000     6.000000     6.00000     8.000000     7.000000  ...   \n",
              "\n",
              "           BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
              "count    2500.000000    2500.000000    2500.000000    2500.000000   \n",
              "mean    43291.162800   38652.274800   37710.555200    6447.848400   \n",
              "std     64202.771958   59876.673254   59390.266423   27426.696364   \n",
              "min    -50616.000000  -53007.000000  -94625.000000       0.000000   \n",
              "25%      1923.000000    1606.500000    1050.000000     969.250000   \n",
              "50%     18474.500000   16520.500000   15830.000000    2100.000000   \n",
              "75%     55975.750000   49844.500000   48498.250000    5010.250000   \n",
              "max    504474.000000  587067.000000  498316.000000  873552.000000   \n",
              "\n",
              "           PAY_AMT2     PAY_AMT3       PAY_AMT4       PAY_AMT5       PAY_AMT6  \\\n",
              "count  2.500000e+03    2500.0000    2500.000000    2500.000000    2500.000000   \n",
              "mean   6.441450e+03    6233.4596    4774.937600    5024.656400    5487.661600   \n",
              "std    3.778192e+04   25715.8828   18620.056749   18159.838678   19088.935962   \n",
              "min    0.000000e+00       0.0000       0.000000       0.000000       0.000000   \n",
              "25%    7.367500e+02     646.7500     286.500000     179.000000     239.500000   \n",
              "50%    2.050000e+03    2000.0000    1500.000000    1500.000000    1600.000000   \n",
              "75%    5.000000e+03    4704.5000    4000.000000    4000.000000    4100.000000   \n",
              "max    1.227082e+06  889043.0000  621000.000000  426529.000000  443001.000000   \n",
              "\n",
              "       default payment next month  \n",
              "count                 2500.000000  \n",
              "mean                     0.216800  \n",
              "std                      0.412148  \n",
              "min                      0.000000  \n",
              "25%                      0.000000  \n",
              "50%                      0.000000  \n",
              "75%                      0.000000  \n",
              "max                      1.000000  \n",
              "\n",
              "[8 rows x 25 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2: Environment definitions and creation\n",
        "So far, we've been using a development environment on the compute instance, your development machine. You'll also need an [environment](https://docs.microsoft.com/azure/machine-learning/concept-environments) to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 2.1: Environment definitions\n",
        "\n",
        "We will make use of Conda .YAML configuration files to create the custom environments:\n",
        "\n",
        "1. Environment #1 for the data cleaning stage of the pipeline - this will handle removing any missing values.\n",
        "2. Environment #2 for the data preparation and model training - as these two steps will require the sklearn library, we will create one environment for them to share."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1710608970634
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing environments/data_clean.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile environments/data_clean.yaml\n",
        "name: prep-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - numpy\n",
        "  - pip\n",
        "  - pandas\n",
        "  - pip:\n",
        "    - mlflow\n",
        "    - azureml-mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1710608980802
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing environments/model_train.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile environments/model_train.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - numpy\n",
        "  - pip\n",
        "  - scikit-learn\n",
        "  - scipy\n",
        "  - pandas\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]\n",
        "    - xlrd\n",
        "    - mlflo\n",
        "    - azureml-mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 2.2: Registering the environments within the workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1713273778977
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name credit-data-clean is registered to workspace, the environment version is 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"credit-data-clean\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    conda_file=os.path.join(\"environments\", \"data_clean.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710609116932
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "custom_env_name = \"credit-model-train\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    conda_file=os.path.join(\"environments\", \"model_train.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 3: Building the pipeline: component definition\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself, using the Azure ML Python SDK v2.\n",
        "\n",
        "Azure ML pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "* Write the specification of the component.\n",
        "* Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "* Load that component from the pipeline code.\n",
        "* Implement the pipeline using this component inputs, outputs and parameters.\n",
        "* Submit the pipeline.\n",
        "\n",
        "In this tutorial we will be defining with our components programmatically using the Python SDK, but there is also the option to do so via YAML file configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 3.2: Programmatically defining the components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Component #1\n",
        "\n",
        "This component handles the cleaning of the data. The cleaning task is performed in the `clean_data.py` Python file. MLFlow will be used to log the parameters and metrics during our pipeline run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1710608885088
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing pipeline_components/clean_data/clean_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pipeline_components/clean_data/clean_data.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--cleaned_data\", type=str, help=\"out path to cleaned data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "    credit_df_raw = pd.read_csv(args.data, header=0)\n",
        "    credit_df_raw.dropna(inplace=True) \n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", credit_df_raw.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", credit_df_raw.shape[1] - 1)\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    credit_df_raw.to_csv(os.path.join(args.cleaned_data, \"cleaned_data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Component #2\n",
        "\n",
        "This component handles the pre-processing of the data, performed in the `prep_data.py` Python file. This script performs the simple task of splitting the data into train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1710608885110
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing pipeline_components/prep_data/prep_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pipeline_components/prep_data/prep_data.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    \n",
        "    credit_df = pd.read_csv(select_first_file(args.data), header=0)\n",
        "\n",
        "    credit_train_df, credit_test_df = train_test_split(\n",
        "        credit_df,\n",
        "        test_size=args.test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Component #3\n",
        "\n",
        "The last component that you'll create will consume the training and test data, train a tree based model and return the output model. You'll use Azure ML logging capabilities to record and visualize the learning progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1710608885143
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing pipeline_components/train_model/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pipeline_components/train_model/train.py\n",
        "import argparse\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_df.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_df.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    clf = GradientBoostingClassifier(\n",
        "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=clf,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=clf,\n",
        "        path=os.path.join(args.model, \"trained_model\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 3.2: Registering the components within the workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620848601
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "src_dir = \"./pipeline_components/clean_data\"\n",
        "\n",
        "data_clean_component = command(\n",
        "    name=\"data_clean_credit_card_defaults\",\n",
        "    display_name=\"Data cleaning for credit training\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_file\")\n",
        "    },\n",
        "    outputs=dict(\n",
        "        cleaned_data=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=src_dir,\n",
        "    command=\"\"\"python clean_data.py \\\n",
        "            --data ${{inputs.data}} --cleaned_data ${{outputs.cleaned_data}}\n",
        "            \"\"\",\n",
        "    environment=\"credit-data-clean:1\",\n",
        ")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "data_clean_component = ml_client.create_or_update(data_clean_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_clean_component.name} with Version {data_clean_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620850798
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "src_dir = \"./pipeline_components/prep_data\"\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_credit_card_defaults\",\n",
        "    display_name=\"Data prep for credit training\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\")\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=src_dir,\n",
        "    command=\"\"\"python prep_data.py \\\n",
        "            --data ${{inputs.data}} --train_data ${{outputs.train_data}} \\\n",
        "            --test_data ${{outputs.test_data}}\n",
        "            \"\"\",\n",
        "    environment=\"credit-model-train:1\",\n",
        ")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620852743
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "src_dir = \"./pipeline_components/train_model\"\n",
        "\n",
        "train_component = command(\n",
        "    name=\"credit_default_model_training\",\n",
        "    display_name=\"Credit defaults model training\",\n",
        "    inputs={\n",
        "        \"train_data\": Input(type=\"uri_folder\"),\n",
        "        \"test_data\": Input(type=\"uri_folder\"),\n",
        "        \"learning_rate\": Input(type=\"number\"),\n",
        "        \"registered_model_name\": Input(type=\"string\")\n",
        "    },\n",
        "    outputs=dict(\n",
        "        model=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=src_dir,\n",
        "    command=\"\"\"python train.py \\\n",
        "            --train_data ${{inputs.train_data}} \\\n",
        "            --test_data ${{inputs.test_data}} \\\n",
        "            --learning_rate ${{inputs.learning_rate}}\n",
        "            \"\"\",\n",
        "    environment=\"credit-model-train:1\",\n",
        ")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 4: Create the pipeline from components\n",
        "Now that all of the components are defined and registered, you can start implementing the pipeline.\n",
        "\n",
        "Here, you'll use input data, split ratio and registered model name as input variables. Then call the components and connect them via their inputs /outputs identifiers. The outputs of each step can be accessed via the .outputs property.\n",
        "\n",
        "To code the pipeline, we use a specific `@dsl.pipeline` decorator that identifies the Azure ML pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs, you can then create multiple instances of a single pipeline with different inputs.\n",
        "\n",
        "Here, we used input data, split ratio and registered model name as input variables. We then call the components and connect them via their inputs /outputs identifiers. The outputs of each step can be accessed via the .outputs property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620853161
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"ej-cluster2\",  # \"serverless\" value runs pipeline on serverless compute\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def credit_defaults_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "\n",
        "    data_clean_job = data_clean_component(\n",
        "        data=pipeline_job_data_input\n",
        "    )\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data=data_clean_job.outputs.cleaned_data,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_cleaned_data\" : data_clean_job.outputs.cleaned_data,\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 5: Define and submit the job\n",
        "It's now time to submit the job to run in Azure ML. This time you'll use `create_or_update` on ml_client.jobs.\n",
        "\n",
        "Here you'll also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
        "\n",
        "Once completed, the pipeline will register a model in your workspace as a result of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620853522
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = credit_defaults_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=credit_data.path),\n",
        "    pipeline_job_learning_rate=0.05,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1710620872936
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"e2e_registered_components\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You can track the progress of your pipeline, by using the link generated in the cell above or in this notebook using the following code:\n",
        "\n",
        "\n",
        "```python\n",
        "    ml_client.jobs.stream(pipeline_job.name)\n",
        "```\n",
        "\n",
        "When you select on each component, you'll see more information about the results of that component. \n",
        "There are two important parts to look for at this stage:\n",
        "* `Outputs+logs` > `user_logs` > `std_log.txt`\n",
        "This section shows the script run sdtout.\n",
        "* `Outputs+logs` > `Metric`\n",
        "This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
